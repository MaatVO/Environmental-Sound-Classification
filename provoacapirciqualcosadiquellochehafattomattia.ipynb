{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for math calculations\n",
    "import numpy as np\n",
    "\n",
    "# import pandas for data (csv) manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# import gc to collect garbage\n",
    "import gc\n",
    "\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# import seaborn for more plotting options(built on top of matplotlib)\n",
    "import seaborn as sns\n",
    "\n",
    "# import librosa for analysing audio signals : visualize audio, display the spectogram\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# import librosa for analysing audio signals : visualize audio, display the spectogram\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "# import wav for reading and writing wav files\n",
    "import wave\n",
    "\n",
    "# import IPython.dispaly for playing audio in Jupter notebook\n",
    "import IPython.display as ipd\n",
    "\n",
    "# import os for system operations\n",
    "import os\n",
    "\n",
    "# import random for get random values/choices\n",
    "import random\n",
    "\n",
    "# import sklearn for machine learning modelling and preprocessing\n",
    "import sklearn\n",
    "\n",
    "# import tensorflow for deep learning modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# importing from sklearn the evaluation metrics for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# importing from sklearn model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "\n",
    "# import tqdm to show a smart progress meter\n",
    "from tqdm.notebook import trange,tqdm\n",
    "\n",
    "# import warnings to hide the unnessairy warniings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"audio\"\n",
    "dataset = pd.read_csv(\"ESC-50-master\\meta\\esc50.csv\")\n",
    "all_files = []\n",
    "for path, subdirs, files in os.walk(data):\n",
    "    for name in files:\n",
    "        all_files.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Modelling üë©üèø"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for RawNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation and Reshape\n",
    "def handle_and_reshape(input_data, target_length=20480, window_size=1024, hop_size=512):\n",
    "    # Split an audio signal into non-silent intervals\n",
    "    segments = librosa.effects.split(input_data, top_db=200000000)\n",
    "\n",
    "    reshaped_data = []\n",
    "    for (start, end) in segments:\n",
    "        segment = input_data[start:end]\n",
    "\n",
    "        if len(segment) < target_length:\n",
    "            # Pad the segment if it's shorter than the target length\n",
    "            padding = target_length - len(segment)\n",
    "            segment = np.pad(segment, (0, padding))\n",
    "        elif len(segment) > target_length:\n",
    "            # Truncate the segment if it's longer than the target length\n",
    "            segment = segment[:target_length]\n",
    "\n",
    "        # Calculate the number of hops based on the window and hop size\n",
    "        num_hops = (len(segment) - window_size) // (hop_size // 2) + 1\n",
    "\n",
    "        for i in range(num_hops):\n",
    "            hop_start = int(i * hop_size // 2)\n",
    "            hop_end = int(hop_start + window_size)\n",
    "\n",
    "            # Ensure each segment has the same length\n",
    "            reshaped_data.append(np.expand_dims(segment[hop_start:hop_end], axis=-1))\n",
    "\n",
    "\n",
    "    return np.array(reshaped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation and Reshape\n",
    "def augment_mel(input_data, target_length=20480, window_size=1024, hop_size=512):\n",
    "    audio = input_data\n",
    "    augmented_audio = time_stretch(audio, factor=0.81)\n",
    "    augmented_audio = pitch_shift(audio, semitones=-2)\n",
    "    augmented_audio = white_noise(audio, noise_level=0.005)\n",
    "    augmented_audio = time_warp(audio, warping_factor=0.3)\n",
    "\n",
    "    segments = librosa.effects.split(augmented_audio, top_db=200000000)\n",
    "    augmented_data = []\n",
    "    for (start, end) in segments:\n",
    "        segment = augmented_audio[start:end]\n",
    "\n",
    "        if len(segment) < target_length:\n",
    "            # Pad the segment if it's shorter than the target length\n",
    "            padding = target_length - len(segment)\n",
    "            segment = np.pad(segment, (0, padding))\n",
    "        elif len(segment) > target_length:\n",
    "            # Truncate the segment if it's longer than the target length\n",
    "            segment = segment[:target_length]\n",
    "\n",
    "        # Calculate the number of hops based on the window and hop size\n",
    "        num_hops = (len(segment) - window_size) // (hop_size // 2) + 1\n",
    "\n",
    "        for i in range(num_hops):\n",
    "            hop_start = int(i * hop_size // 2)\n",
    "            hop_end = int(hop_start + window_size)\n",
    "\n",
    "            # Ensure each segment has the same length\n",
    "            #augmented_data.append(np.expand_dims(segment[hop_start:hop_end], axis=-1))\n",
    "\n",
    "\n",
    "    return np.array(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch(audio, factor):\n",
    "    # Convert NumPy array to AudioSegment\n",
    "    audio_segment = AudioSegment(audio.tobytes(), frame_rate=22050, sample_width=audio.dtype.itemsize, channels=1)\n",
    "    # Apply time stretching\n",
    "    stretched_audio = audio_segment.speedup(playback_speed=factor)\n",
    "    # Convert back to NumPy array\n",
    "    stretched_array = np.array(stretched_audio.get_array_of_samples())\n",
    "    return stretched_array\n",
    "\n",
    "def pitch_shift(audio, semitones):\n",
    "    # Convert NumPy array to AudioSegment\n",
    "    audio_segment = AudioSegment(audio.tobytes(), frame_rate=22050, sample_width=audio.dtype.itemsize, channels=1)\n",
    "    # Apply pitch shifting\n",
    "    shifted_audio = audio_segment._spawn(audio_segment.raw_data, overrides={\n",
    "        \"frame_rate\": int(audio_segment.frame_rate * (2 ** (semitones / 12.0)))\n",
    "    })\n",
    "    # Convert back to NumPy array\n",
    "    shifted_array = np.array(shifted_audio.get_array_of_samples())\n",
    "    return shifted_array\n",
    "\n",
    "def white_noise(audio, noise_level=0.005):\n",
    "    # Add white noise\n",
    "    noise = np.random.normal(0, noise_level, len(audio))\n",
    "    audio_with_noise = audio + noise\n",
    "    # Clip values to stay within the valid range for int16 audio\n",
    "    audio_with_noise = np.clip(audio_with_noise, -32768, 32767).astype(np.int16)\n",
    "    return audio_with_noise\n",
    "\n",
    "def time_warp(audio, warping_factor):\n",
    "    num_frames = len(audio)\n",
    "    frame_indices = np.arange(0, num_frames)\n",
    "    warped_indices = np.int16(frame_indices + warping_factor * np.sin(np.linspace(0, 2*np.pi, num_frames)))\n",
    "\n",
    "    # Ensure that the warped indices are within the valid range\n",
    "    warped_indices = np.clip(warped_indices, 0, num_frames - 1)\n",
    "\n",
    "    # Apply time warping\n",
    "    warped_audio = audio[warped_indices]\n",
    "\n",
    "    return warped_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_log_mel_features(audio, sr):\n",
    "    # Load audio file\n",
    "    y = audio\n",
    "\n",
    "    # Extract log-scaled mel spectrogram features\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=22050, n_mels=60)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "    # Divide the sound segments into frames with overlap\n",
    "    frames = librosa.util.frame(log_mel_spectrogram, frame_length=23, hop_length=11)\n",
    "\n",
    "    # Calculate the first temporal derivative\n",
    "    delta_log_mel = librosa.feature.delta(log_mel_spectrogram)\n",
    "\n",
    "    # Combine static and temporal derivative features\n",
    "    feature_map = np.stack([log_mel_spectrogram, delta_log_mel], axis=-1)\n",
    "\n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72408ccde6c54a1f84703fc4bda64311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    audio_path = os.path.join('ESC-50-master/audio', dataset['filename'][i])\n",
    "    audio,_ = librosa.load(audio_path, sr=22050, mono=True)\n",
    "    #segments = augment_mel(audio)\n",
    "    spettrogramma = extract_log_mel_features(audio, _)#logmel, delta = process_audio_file(segments)\n",
    "    X.append(spettrogramma)\n",
    "    y.append(dataset['target'][i])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 60, 216, 2) (400, 60, 216, 2) (1600, 50) (400, 50)\n"
     ]
    }
   ],
   "source": [
    "# Save X and y to CSV\n",
    "df = pd.DataFrame(data={'label': y, 'feature': list(X)})\n",
    "df.to_csv('data_logmel.csv', index=False)\n",
    "X_train_mel, X_test_mel, y_train_mel, y_test_mel = train_test_split(X, to_categorical(y, num_classes=50), test_size=0.2, random_state=42)\n",
    "print(X_train_mel.shape, X_test_mel.shape, y_train_mel.shape, y_test_mel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            restore_best_weights = True,\n",
    "                                            patience=10,\n",
    "                                            verbose=1)\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Define the dynamic learning rate schedule\n",
    "def lr_schedule(epoch, lr):\n",
    "    min_learning_rate = 0.0001\n",
    "    max_learning_rate = 0.001\n",
    "    decay_speed = 5.0\n",
    "    return min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-epoch / decay_speed)\n",
    "\n",
    "initial_learning_rate = 0.003\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RawNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 60, 216, 24)       888       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 216, 24)       20760     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 30, 108, 48)       28848     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 15, 54, 48)        57648     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 27, 64)         49216     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 8, 27, 64)        256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 13824)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               2765000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                10050     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,932,666\n",
      "Trainable params: 2,932,538\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# let's build rawnet CNN in tensorflow\n",
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# reset the memory of tensorflow\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "df = pd.read_csv('ESC-50-master/meta/esc50.csv')\n",
    "audio_directory = 'ESC-50-master/audio'\n",
    "labels = df.set_index('filename')['target'].to_dict()\n",
    "num_classes = len(set(labels.values()))\n",
    "input_shape = (X_train_mel.shape[1], X_train_mel.shape[2], 1)\n",
    "\n",
    "rawnet_model = Sequential([\n",
    "    tf.keras.layers.Conv2D(24, (6, 6), padding='same',strides = (1,1), input_shape=input_shape, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(24, (6, 6), padding='same',strides = (1,1), input_shape=input_shape, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(48, (5, 5), padding='same',strides = (2,2), input_shape=input_shape, activation= 'relu'),\n",
    "    tf.keras.layers.Conv2D(48, (5, 5), padding='same',strides = (2,2), input_shape=input_shape, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, (4, 4), padding='same',strides = (2,2), input_shape=input_shape, activation='relu'),\n",
    "\n",
    "    # batch normalization layer to improve the learning process\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    # fully connected layer with 200 units\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    # applying dropout to prevent overfitting as described in the paper\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # output layer with 50 units\n",
    "    tf.keras.layers.Dense(50, activation='softmax')\n",
    "])\n",
    "\n",
    "rawnet_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
