{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>100032</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>chirping_birds</td>\n",
       "      <td>False</td>\n",
       "      <td>100038</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>vacuum_cleaner</td>\n",
       "      <td>False</td>\n",
       "      <td>100210</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>False</td>\n",
       "      <td>101296</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  fold  target        category  esc10  src_file take\n",
       "0   1-100032-A-0.wav     1       0             dog   True    100032    A\n",
       "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n",
       "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n",
       "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n",
       "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ESC-50\\ESC-50-master\\meta\\esc50.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "fold = []\n",
    "target = []\n",
    "category = []\n",
    "src_file = []\n",
    "take = []\n",
    "for i in df.columns:\n",
    "    if i == 'filename':\n",
    "        filenames.append(df[i])\n",
    "    if i == 'fold':\n",
    "        fold.append(df[i])\n",
    "    if i == 'target':\n",
    "        target.append(df[i])\n",
    "    if i == 'category':\n",
    "        category.append(df[i])\n",
    "    if i == 'src_file':\n",
    "        src_file.append(df[i])\n",
    "    if i == 'take':\n",
    "        take.append(df[i])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydub\n",
    "import librosa\n",
    "\n",
    "class Clip:\n",
    "    \"\"\"A single 5-sec long recording.\"\"\"\n",
    "    \n",
    "    RATE = 44100   # All recordings in ESC are 44.1 kHz\n",
    "    FRAME = 512    # Frame size in samples\n",
    "    \n",
    "    class Audio:\n",
    "        \"\"\"The actual audio data of the clip.\n",
    "        \n",
    "            Uses a context manager to load/unload the raw audio data. This way clips\n",
    "            can be processed sequentially with reasonable memory usage.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, path):\n",
    "            self.path = path\n",
    "        \n",
    "        def __enter__(self):\n",
    "            # Actual recordings are sometimes not frame accurate, so we trim/overlay to exactly 5 seconds\n",
    "            self.data = pydub.AudioSegment.silent(duration=5000)\n",
    "            self.data = self.data.overlay(pydub.AudioSegment.from_file(self.path)[0:5000])\n",
    "            self.raw = (np.frombuffer(self.data._data, dtype=\"int16\") + 0.5) / (0x7FFF + 0.5)   # convert to float\n",
    "\n",
    "            return(self)\n",
    "        \n",
    "        \n",
    "        def __exit__(self, exception_type, exception_value, traceback):\n",
    "            if exception_type is not None:\n",
    "                print (exception_type, exception_value, traceback)\n",
    "            del self.data\n",
    "            del self.raw\n",
    "        \n",
    "    def __init__(self, filename):\n",
    "        self.filename = os.path.basename(filename)\n",
    "        self.path = os.path.abspath(filename)        \n",
    "        self.directory = os.path.dirname(self.path)\n",
    "        self.category = self.directory.split('/')[-1]\n",
    "        \n",
    "        self.audio = Clip.Audio(self.path)\n",
    "        \n",
    "        with self.audio as audio:\n",
    "            self._compute_mfcc(audio)    \n",
    "            self._compute_zcr(audio)\n",
    "            \n",
    "    def _compute_mfcc(self, audio):\n",
    "        # MFCC computation with default settings (2048 FFT window length, 512 hop length, 128 bands)\n",
    "        self.melspectrogram = librosa.feature.melspectrogram(y = audio.raw, sr=Clip.RATE, hop_length=Clip.FRAME)\n",
    "        self.logamplitude = librosa.amplitude_to_db(self.melspectrogram)\n",
    "        self.mfcc = librosa.feature.mfcc(S=self.logamplitude, n_mfcc=13).transpose()\n",
    "            \n",
    "    def _compute_zcr(self, audio):\n",
    "        # Zero-crossing rate\n",
    "        self.zcr = []\n",
    "        frames = int(np.ceil(len(audio.data) / 1000.0 * Clip.RATE / Clip.FRAME))\n",
    "        \n",
    "        for i in range(0, frames):\n",
    "            frame = Clip._get_frame(audio, i)\n",
    "            self.zcr.append(np.mean(0.5 * np.abs(np.diff(np.sign(frame)))))\n",
    "\n",
    "        self.zcr = np.asarray(self.zcr)\n",
    "            \n",
    "    @classmethod\n",
    "    def _get_frame(cls, audio, index):\n",
    "        if index < 0:\n",
    "            return None\n",
    "        return audio.raw[(index * Clip.FRAME):(index+1) * Clip.FRAME]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '<{0}/{1}>'.format(self.category, self.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "class AudioUtil:\n",
    "    @staticmethod\n",
    "    def open(file_path):\n",
    "        # Use librosa to load audio file\n",
    "        audio, _ = librosa.load(file_path, sr=None)\n",
    "        return audio\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(audio, target_sr):\n",
    "        # Use librosa to resample audio\n",
    "        return librosa.resample(audio, orig_sr=len(audio), target_sr=target_sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(audio, target_channels):\n",
    "        # If the number of channels is already correct, return the input\n",
    "        if audio.shape[0] == target_channels:\n",
    "            return audio\n",
    "        # If not, duplicate the channels to match the target_channels\n",
    "        return np.tile(audio, target_channels)[:target_channels]\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(audio, target_length):\n",
    "        # Use librosa to pad or truncate audio to the target length\n",
    "        if len(audio) < target_length:\n",
    "            return np.pad(audio, (0, target_length - len(audio)))\n",
    "        else:\n",
    "            return audio[:target_length]\n",
    "\n",
    "    @staticmethod\n",
    "    def time_shift(audio, shift_pct):\n",
    "        # Use numpy to time-shift the audio\n",
    "        shift_amount = int(len(audio) * shift_pct)\n",
    "        return np.roll(audio, shift_amount)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectrogram(audio, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        # Use librosa to compute the mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(audio, sr=len(audio), n_mels=n_mels, n_fft=n_fft, hop_length=hop_len)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return log_mel_spec\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spectrogram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2):\n",
    "        # Use scipy to apply frequency and time masking\n",
    "        _, n_frames = spectrogram.shape\n",
    "        freq_mask_size = int(n_freq_masks * max_mask_pct * n_frames)\n",
    "        time_mask_size = int(n_time_masks * max_mask_pct * n_frames)\n",
    "\n",
    "        freq_mask = signal.windows.hann(freq_mask_size)\n",
    "        time_mask = signal.windows.hann(time_mask_size)\n",
    "\n",
    "        augmented_spec = spectrogram.copy()\n",
    "\n",
    "        for _ in range(n_freq_masks):\n",
    "            f = np.random.randint(0, n_frames - freq_mask_size)\n",
    "            augmented_spec[:, f:f + freq_mask_size] *= freq_mask\n",
    "\n",
    "        for _ in range(n_time_masks):\n",
    "            t = np.random.randint(0, n_frames - time_mask_size)\n",
    "            augmented_spec[:, t:t + time_mask_size] *= time_mask\n",
    "\n",
    "        return augmented_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sb\n",
    "sb.set(style=\"white\", palette=\"muted\")\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(20150420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 4000\n",
    "    self.sr = 44000\n",
    "    self.channel = 1\n",
    "    self.shift_pct = 0.4\n",
    "\n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # Relative file path of the audio file with 'dataset/' prefix\n",
    "    if idx < 0 or idx >= len(self.df):\n",
    "        # Handle cases where the index is out of range\n",
    "        raise IndexError(f\"Index {idx} is out of range for the DataFrame\")\n",
    "\n",
    "    filename = self.df.loc[idx, 'filename']\n",
    "    # Construct the full path to the audio file by joining with the 'dataset' directory\n",
    "    full_path = os.path.join(self.data_path, filename)\n",
    "\n",
    "    # Get the Class ID\n",
    "    class_id = self.df.loc[idx, 'target']  # Assuming 'target' is the class ID\n",
    "\n",
    "    return full_path, class_id\n",
    "\n",
    "    # Load and process the audio file\n",
    "    aud = AudioUtil.open(full_path)\n",
    "\n",
    "\n",
    "    # Perform audio processing as before\n",
    "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "    # majority. So make all sounds have the same number of channels and same\n",
    "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "    # result in arrays of different lengths, even though the sound duration is\n",
    "    # the same.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, class_id\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\matti\\\\Documents\\\\GitHub\\\\HDA_project'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'ESC-50/audio'\n",
    "\n",
    "myds = SoundDS(df, data_dir)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# creating train test split manually for further usage\n",
    "train_X = []\n",
    "train_y = []\n",
    "for tensor in range(len(train_ds)):\n",
    "    train_X.append(train_ds[tensor][0])\n",
    "    train_y.append(train_ds[tensor][1])\n",
    "\n",
    "test_X = []\n",
    "test_y = []\n",
    "for tensor in range(len(val_ds)):\n",
    "    test_X.append(val_ds[tensor][0])\n",
    "    test_y.append(val_ds[tensor][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
