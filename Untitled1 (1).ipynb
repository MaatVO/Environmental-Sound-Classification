{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnoGfZTFOqS0",
        "outputId": "3d69f2a3-5ff6-487a-a01e-2f83e6905bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd 'drive/MyDrive/HDA'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-jKVzm2Pheq",
        "outputId": "d027f540-233c-4192-d061-408f2c07c211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from pathlib import Path\n",
        "import os\n",
        "import math, random\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "from IPython.display import Audio\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "tuwEnbl5Orzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('df.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "x6b64nJIyAWw",
        "outputId": "fe0777c7-a684-4459-8da6-2c403f622b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      relative_path  classID\n",
              "0      kick-001.wav        4\n",
              "1  kick-001-a-1.wav        4\n",
              "2  kick-001-a-2.wav        4\n",
              "3  kick-001-a-3.wav        4\n",
              "4  kick-001-a-4.wav        4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c316d86f-4ab7-4d08-9f6f-298a518cf94f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relative_path</th>\n",
              "      <th>classID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kick-001.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kick-001-a-1.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>kick-001-a-2.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kick-001-a-3.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kick-001-a-4.wav</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c316d86f-4ab7-4d08-9f6f-298a518cf94f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c316d86f-4ab7-4d08-9f6f-298a518cf94f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c316d86f-4ab7-4d08-9f6f-298a518cf94f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8700005c-6410-4b81-ba6b-60ff7dffc56c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8700005c-6410-4b81-ba6b-60ff7dffc56c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8700005c-6410-4b81-ba6b-60ff7dffc56c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see how many samples for each class we have\n",
        "df['classID'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ1RRY1M3H-V",
        "outputId": "3d9da8d7-d5cc-41dd-8d6c-df5b1535dfdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    1735\n",
              "2    1545\n",
              "5    1515\n",
              "0     677\n",
              "3     161\n",
              "Name: classID, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's reduce the number of samples for the class number 5 4 2 by 1000\n",
        "df = df.drop(df[df['classID'] == 5].sample(1000).index)\n",
        "df = df.drop(df[df['classID'] == 4].sample(1000).index)\n",
        "df = df.drop(df[df['classID'] == 2].sample(1000).index)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# let's see how many samples for each class we have\n",
        "df['classID'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-cOvCL04BJ2",
        "outputId": "e730a9ae-5640-4e75-f930-d5c5bce1852e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    735\n",
              "0    677\n",
              "2    545\n",
              "5    515\n",
              "3    161\n",
              "Name: classID, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioUtil():\n",
        "  # ----------------------------\n",
        "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def open(audio_file):\n",
        "    try:\n",
        "        sig, sr = torchaudio.load(audio_file)\n",
        "        return (sig, sr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening {audio_file}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Since Resample applies to a single channel, we resample one channel at a time\n",
        "  # ----------------------------\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert the given audio to the desired number of channels\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def rechannel(aud, new_channel):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sig.shape[0] == new_channel):\n",
        "      # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    if (new_channel == 1):\n",
        "      # Convert from stereo to mono by selecting only the first channel\n",
        "      resig = sig[:1, :]\n",
        "    else:\n",
        "      # Convert from mono to stereo by duplicating the first channel\n",
        "      resig = torch.cat([sig, sig])\n",
        "\n",
        "    return ((resig, sr))\n",
        "\n",
        "  @staticmethod\n",
        "  def resample(aud, newsr):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sr == newsr):\n",
        "      # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    num_channels = sig.shape[0]\n",
        "    # Resample first channel\n",
        "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
        "    if (num_channels > 1):\n",
        "      # Resample the second channel and merge both channels\n",
        "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
        "      resig = torch.cat([resig, retwo])\n",
        "\n",
        "    return ((resig, newsr))\n",
        "\n",
        "# ----------------------------\n",
        "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def pad_trunc(aud, max_ms):\n",
        "    sig, sr = aud\n",
        "    num_rows, sig_len = sig.shape\n",
        "    max_len = sr//1000 * max_ms\n",
        "\n",
        "    if (sig_len > max_len):\n",
        "      # Truncate the signal to the given length\n",
        "      sig = sig[:,:max_len]\n",
        "\n",
        "    elif (sig_len < max_len):\n",
        "      # Length of padding to add at the beginning and end of the signal\n",
        "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
        "      pad_end_len = max_len - sig_len - pad_begin_len\n",
        "\n",
        "      # Pad with 0s\n",
        "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "\n",
        "    return (sig, sr)\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Shifts the signal to the left or right by some percent. Values at the end\n",
        "  # are 'wrapped around' to the start of the transformed signal.\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def time_shift(aud, shift_limit):\n",
        "    sig,sr = aud\n",
        "    _, sig_len = sig.shape\n",
        "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
        "    return (sig.roll(shift_amt), sr)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Generate a Spectrogram\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
        "    sig,sr = aud\n",
        "    top_db = 80\n",
        "\n",
        "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
        "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
        "\n",
        "    # Convert to decibels\n",
        "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "    return (spec)\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
        "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
        "  # overfitting and to help the model generalise better. The masked sections are\n",
        "  # replaced with the mean value.\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "    _, n_mels, n_steps = spec.shape\n",
        "    mask_value = spec.mean()\n",
        "    aug_spec = spec\n",
        "\n",
        "    freq_mask_param = max_mask_pct * n_mels\n",
        "    for _ in range(n_freq_masks):\n",
        "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "    time_mask_param = max_mask_pct * n_steps\n",
        "    for _ in range(n_time_masks):\n",
        "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
        "\n",
        "    return aug_spec"
      ],
      "metadata": {
        "id": "SvfLeAu8PtIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchaudio\n",
        "\n",
        "# ----------------------------\n",
        "# Sound Dataset\n",
        "# ----------------------------\n",
        "class SoundDS(Dataset):\n",
        "  def __init__(self, df, data_path):\n",
        "    self.df = df\n",
        "    self.data_path = str(data_path)\n",
        "    self.duration = 4000\n",
        "    self.sr = 16000\n",
        "    self.channel = 1\n",
        "    self.shift_pct = 0.4\n",
        "\n",
        "  # ----------------------------\n",
        "  # Number of items in dataset\n",
        "  # ----------------------------\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get i'th item in dataset\n",
        "  # ----------------------------\n",
        "  def __getitem__(self, idx):\n",
        "    # Absolute file path of the audio file - concatenate the audio directory with\n",
        "    # Relative file path of the audio file with 'dataset/' prefix\n",
        "    if idx < 0 or idx >= len(self.df):\n",
        "            # Handle cases where the index is out of range\n",
        "            raise IndexError(f\"Index {idx} is out of range for the DataFrame\")\n",
        "\n",
        "    relative_path = self.df.loc[idx, 'relative_path']\n",
        "    # Construct the full path to the audio file by joining with the 'dataset' directory\n",
        "    full_path = os.path.join(self.data_path, relative_path)\n",
        "\n",
        "\n",
        "\n",
        "    # Get the Class ID\n",
        "    class_id = self.df.loc[idx, 'classID']\n",
        "\n",
        "    # Load and process the audio file\n",
        "    aud = AudioUtil.open(full_path)\n",
        "\n",
        "\n",
        "    # Perform audio processing as before\n",
        "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
        "    # majority. So make all sounds have the same number of channels and same\n",
        "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
        "    # result in arrays of different lengths, even though the sound duration is\n",
        "    # the same.\n",
        "    reaud = AudioUtil.resample(aud, self.sr)\n",
        "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
        "\n",
        "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
        "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
        "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
        "\n",
        "    return aug_sgram, class_id\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sI8NAHygQDXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIZky1AWTCJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "data_dir = 'dataset/'\n",
        "\n",
        "myds = SoundDS(df, data_dir)\n",
        "\n",
        "# Random split of 80:20 between training and validation\n",
        "num_items = len(myds)\n",
        "num_train = round(num_items * 0.8)\n",
        "num_val = num_items - num_train\n",
        "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
        "\n",
        "# creating train test split manually for further usage\n",
        "train_X = []\n",
        "train_y = []\n",
        "for tensor in range(len(train_ds)):\n",
        "    train_X.append(train_ds[tensor][0])\n",
        "    train_y.append(train_ds[tensor][1])\n",
        "\n",
        "test_X = []\n",
        "test_y = []\n",
        "for tensor in range(len(val_ds)):\n",
        "    test_X.append(val_ds[tensor][0])\n",
        "    test_y.append(val_ds[tensor][1])\n",
        "\n",
        "\n",
        "\n",
        "df.to_csv('df.csv', index=False)\n",
        "\n",
        "# Create training and validation data loaders, for torch\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GF2A7RmXQExJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
        "        self.conv4.bias.data.zero_()\n",
        "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel = AudioClassifier()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel = myModel.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcTMe4VpQIzb",
        "outputId": "3c460080-eee3-4781-e9e6-429f694fbb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "\n",
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training(model, train_dl, num_epochs):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
        "                                                steps_per_epoch=int(len(train_dl)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "\n",
        "  # Repeat for each epoch\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Repeat for each batch in the training set\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Keep stats for Loss and Accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Get the predicted class with the highest score\n",
        "        _, prediction = torch.max(outputs,1)\n",
        "        # Count of predictions that matched the target label\n",
        "        correct_prediction += (prediction == labels).sum().item()\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "        #if i % 10 == 0:    # print every 10 mini-batches\n",
        "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
        "\n",
        "    # Print stats at the end of the epoch\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    acc = correct_prediction/total_prediction\n",
        "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
        "\n",
        "  print('Finished Training')\n",
        "\n",
        "num_epochs=20    # adjust this.\n",
        "training(myModel, train_dl, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC_CtQg_QUEf",
        "outputId": "53ec1aab-8f67-4498-ab10-d52808614f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.94, Accuracy: 0.34\n",
            "Epoch: 1, Loss: 1.30, Accuracy: 0.54\n",
            "Epoch: 2, Loss: 1.11, Accuracy: 0.57\n",
            "Epoch: 3, Loss: 1.00, Accuracy: 0.60\n",
            "Epoch: 4, Loss: 0.90, Accuracy: 0.64\n",
            "Epoch: 5, Loss: 0.82, Accuracy: 0.67\n",
            "Epoch: 6, Loss: 0.73, Accuracy: 0.71\n",
            "Epoch: 7, Loss: 0.70, Accuracy: 0.72\n",
            "Epoch: 8, Loss: 0.67, Accuracy: 0.73\n",
            "Epoch: 9, Loss: 0.66, Accuracy: 0.72\n",
            "Epoch: 10, Loss: 0.63, Accuracy: 0.74\n",
            "Epoch: 11, Loss: 0.61, Accuracy: 0.75\n",
            "Epoch: 12, Loss: 0.57, Accuracy: 0.77\n",
            "Epoch: 13, Loss: 0.57, Accuracy: 0.75\n",
            "Epoch: 14, Loss: 0.55, Accuracy: 0.77\n",
            "Epoch: 15, Loss: 0.54, Accuracy: 0.78\n",
            "Epoch: 16, Loss: 0.53, Accuracy: 0.78\n",
            "Epoch: 17, Loss: 0.52, Accuracy: 0.78\n",
            "Epoch: 18, Loss: 0.50, Accuracy: 0.79\n",
            "Epoch: 19, Loss: 0.50, Accuracy: 0.80\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rzpUy_DyRz-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3ts_TWB_R8I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "    \"\"\"Generates RNN-LSTM model\n",
        "\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: RNN-LSTM model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 2 LSTM layers\n",
        "    model.add(keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "\n",
        "    # dense layer\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZXgfc48rVmz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create network\n",
        "\n",
        "input_shape = (64, 126) # 1, 64\n",
        "model = build_model(input_shape)\n",
        "\n",
        "# compile model\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# train model\n",
        "history = model.fit(train_X, train_y, batch_size=32, epochs=30)\n",
        "\n",
        "# plot accuracy/error for training and validation\n",
        "plot_history(history)\n",
        "\n",
        "# evaluate model on test set\n",
        "test_loss, test_acc = model.evaluate(test_X, test_y, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "B4p8ZXyrVrGn",
        "outputId": "a4d74ca1-a227-4f80-cfbb-9b4137fbbab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_14 (LSTM)              (None, 64, 64)            48896     \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86730 (338.79 KB)\n",
            "Trainable params: 86730 (338.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-143-1cbe428620e8>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# plot accuracy/error for training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1106\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1107\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'torch.Tensor'>\"}), (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"})"
          ]
        }
      ]
    }
  ]
}