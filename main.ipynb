{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMhID66ramoF7FEK6WHX2G2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnoGfZTFOqS0","executionInfo":{"status":"ok","timestamp":1699274188751,"user_tz":-60,"elapsed":42871,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}},"outputId":"67df4e15-9132-4b4c-e743-601d0d7f91d9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd 'drive/MyDrive/HDA'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-jKVzm2Pheq","executionInfo":{"status":"ok","timestamp":1699274188752,"user_tz":-60,"elapsed":6,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}},"outputId":"594907ac-33b5-4a7b-99d8-9549fe219d21"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/HDA\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib as plt\n","from pathlib import Path\n","import os\n","import math, random\n","import torch\n","import torchaudio\n","from torchaudio import transforms\n","from IPython.display import Audio\n","\n","from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"tuwEnbl5Orzi","executionInfo":{"status":"ok","timestamp":1699274197248,"user_tz":-60,"elapsed":8499,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('df.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"x6b64nJIyAWw","executionInfo":{"status":"ok","timestamp":1699274198265,"user_tz":-60,"elapsed":1021,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}},"outputId":"e096afa3-2948-43d0-aaff-2a145dcc7ccd"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      relative_path  classID\n","0      kick-001.wav        4\n","1  kick-001-a-1.wav        4\n","2  kick-001-a-2.wav        4\n","3  kick-001-a-3.wav        4\n","4  kick-001-a-4.wav        4"],"text/html":["\n","  <div id=\"df-41b7ad1d-b888-4c81-ad57-267738f74503\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>relative_path</th>\n","      <th>classID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kick-001.wav</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>kick-001-a-1.wav</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kick-001-a-2.wav</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>kick-001-a-3.wav</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kick-001-a-4.wav</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41b7ad1d-b888-4c81-ad57-267738f74503')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-41b7ad1d-b888-4c81-ad57-267738f74503 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-41b7ad1d-b888-4c81-ad57-267738f74503');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5a34fed1-38ad-4877-b28f-3cf56eb2e305\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a34fed1-38ad-4877-b28f-3cf56eb2e305')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5a34fed1-38ad-4877-b28f-3cf56eb2e305 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["class AudioUtil():\n","  # ----------------------------\n","  # Load an audio file. Return the signal as a tensor and the sample rate\n","  # ----------------------------\n","  @staticmethod\n","  def open(audio_file):\n","    try:\n","        sig, sr = torchaudio.load(audio_file)\n","        return (sig, sr)\n","    except Exception as e:\n","        print(f\"Error opening {audio_file}: {str(e)}\")\n","        return None\n","\n","\n","  # ----------------------------\n","  # Since Resample applies to a single channel, we resample one channel at a time\n","  # ----------------------------\n","\n","  # ----------------------------\n","  # Convert the given audio to the desired number of channels\n","  # ----------------------------\n","  @staticmethod\n","  def rechannel(aud, new_channel):\n","    sig, sr = aud\n","\n","    if (sig.shape[0] == new_channel):\n","      # Nothing to do\n","      return aud\n","\n","    if (new_channel == 1):\n","      # Convert from stereo to mono by selecting only the first channel\n","      resig = sig[:1, :]\n","    else:\n","      # Convert from mono to stereo by duplicating the first channel\n","      resig = torch.cat([sig, sig])\n","\n","    return ((resig, sr))\n","\n","  @staticmethod\n","  def resample(aud, newsr):\n","    sig, sr = aud\n","\n","    if (sr == newsr):\n","      # Nothing to do\n","      return aud\n","\n","    num_channels = sig.shape[0]\n","    # Resample first channel\n","    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n","    if (num_channels > 1):\n","      # Resample the second channel and merge both channels\n","      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n","      resig = torch.cat([resig, retwo])\n","\n","    return ((resig, newsr))\n","\n","# ----------------------------\n","  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n","  # ----------------------------\n","  @staticmethod\n","  def pad_trunc(aud, max_ms):\n","    sig, sr = aud\n","    num_rows, sig_len = sig.shape\n","    max_len = sr//1000 * max_ms\n","\n","    if (sig_len > max_len):\n","      # Truncate the signal to the given length\n","      sig = sig[:,:max_len]\n","\n","    elif (sig_len < max_len):\n","      # Length of padding to add at the beginning and end of the signal\n","      pad_begin_len = random.randint(0, max_len - sig_len)\n","      pad_end_len = max_len - sig_len - pad_begin_len\n","\n","      # Pad with 0s\n","      pad_begin = torch.zeros((num_rows, pad_begin_len))\n","      pad_end = torch.zeros((num_rows, pad_end_len))\n","\n","      sig = torch.cat((pad_begin, sig, pad_end), 1)\n","\n","    return (sig, sr)\n","\n","\n","  # ----------------------------\n","  # Shifts the signal to the left or right by some percent. Values at the end\n","  # are 'wrapped around' to the start of the transformed signal.\n","  # ----------------------------\n","  @staticmethod\n","  def time_shift(aud, shift_limit):\n","    sig,sr = aud\n","    _, sig_len = sig.shape\n","    shift_amt = int(random.random() * shift_limit * sig_len)\n","    return (sig.roll(shift_amt), sr)\n","\n","  # ----------------------------\n","  # Generate a Spectrogram\n","  # ----------------------------\n","  @staticmethod\n","  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n","    sig,sr = aud\n","    top_db = 80\n","\n","    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n","    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n","\n","    # Convert to decibels\n","    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n","    return (spec)\n","\n","\n","  # ----------------------------\n","  # Augment the Spectrogram by masking out some sections of it in both the frequency\n","  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n","  # overfitting and to help the model generalise better. The masked sections are\n","  # replaced with the mean value.\n","  # ----------------------------\n","  @staticmethod\n","  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n","    _, n_mels, n_steps = spec.shape\n","    mask_value = spec.mean()\n","    aug_spec = spec\n","\n","    freq_mask_param = max_mask_pct * n_mels\n","    for _ in range(n_freq_masks):\n","      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n","\n","    time_mask_param = max_mask_pct * n_steps\n","    for _ in range(n_time_masks):\n","      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n","\n","    return aug_spec"],"metadata":{"id":"SvfLeAu8PtIl","executionInfo":{"status":"ok","timestamp":1699274202437,"user_tz":-60,"elapsed":489,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset, random_split\n","import torchaudio\n","\n","# ----------------------------\n","# Sound Dataset\n","# ----------------------------\n","class SoundDS(Dataset):\n","  def __init__(self, df, data_path):\n","    self.df = df\n","    self.data_path = str(data_path)\n","    self.duration = 4000\n","    self.sr = 16000\n","    self.channel = 1\n","    self.shift_pct = 0.4\n","\n","  # ----------------------------\n","  # Number of items in dataset\n","  # ----------------------------\n","  def __len__(self):\n","    return len(self.df)\n","\n","  # ----------------------------\n","  # Get i'th item in dataset\n","  # ----------------------------\n","  def __getitem__(self, idx):\n","    # Absolute file path of the audio file - concatenate the audio directory with\n","    # Relative file path of the audio file with 'dataset/' prefix\n","    if idx < 0 or idx >= len(self.df):\n","            # Handle cases where the index is out of range\n","            raise IndexError(f\"Index {idx} is out of range for the DataFrame\")\n","\n","    relative_path = self.df.loc[idx, 'relative_path']\n","    # Construct the full path to the audio file by joining with the 'dataset' directory\n","    full_path = os.path.join(self.data_path, relative_path)\n","\n","\n","\n","    # Get the Class ID\n","    class_id = self.df.loc[idx, 'classID']\n","\n","    # Load and process the audio file\n","    aud = AudioUtil.open(full_path)\n","\n","\n","    # Perform audio processing as before\n","    # Some sounds have a higher sample rate, or fewer channels compared to the\n","    # majority. So make all sounds have the same number of channels and same\n","    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n","    # result in arrays of different lengths, even though the sound duration is\n","    # the same.\n","    reaud = AudioUtil.resample(aud, self.sr)\n","    rechan = AudioUtil.rechannel(reaud, self.channel)\n","\n","    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n","    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n","    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n","    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n","\n","    return aug_sgram, class_id\n","\n","\n","\n","\n"],"metadata":{"id":"sI8NAHygQDXe","executionInfo":{"status":"ok","timestamp":1699274210441,"user_tz":-60,"elapsed":800,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import random_split\n","\n","data_dir = 'dataset/'\n","\n","myds = SoundDS(df, data_dir)\n","\n","# Random split of 80:20 between training and validation\n","num_items = len(myds)\n","num_train = round(num_items * 0.8)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(myds, [num_train, num_val])\n","\n","df.to_csv('df.csv', index=False)\n","\n","# Create training and validation data loaders\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False)\n","val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"],"metadata":{"id":"GF2A7RmXQExJ","executionInfo":{"status":"ok","timestamp":1699274216257,"user_tz":-60,"elapsed":449,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch.nn import init\n","import torch.nn as nn\n","# ----------------------------\n","# Audio Classification Model\n","# ----------------------------\n","class AudioClassifier (nn.Module):\n","    # ----------------------------\n","    # Build the model architecture\n","    # ----------------------------\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(8)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","\n","        # Second Convolution Block\n","        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","\n","        # Second Convolution Block\n","        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=64, out_features=10)\n","\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n","\n","    # ----------------------------\n","    # Forward pass computations\n","    # ----------------------------\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        # Linear layer\n","        x = self.lin(x)\n","\n","        # Final output\n","        return x\n","\n","# Create the model and put it on the GPU if available\n","myModel = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","myModel = myModel.to(device)\n","# Check that it is on Cuda\n","next(myModel.parameters()).device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcTMe4VpQIzb","executionInfo":{"status":"ok","timestamp":1699274226398,"user_tz":-60,"elapsed":5516,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}},"outputId":"88696682-d2a7-406c-cb76-d88088576311"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import soundfile as sf\n","\n","# ----------------------------\n","# Training Loop\n","# ----------------------------\n","def training(model, train_dl, num_epochs):\n","  # Loss Function, Optimizer and Scheduler\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","\n","  # Repeat for each epoch\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","\n","    # Repeat for each batch in the training set\n","    for i, data in enumerate(train_dl):\n","\n","        # Get the input features and target labels, and put them on the GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # Normalize the inputs\n","        inputs_m, inputs_s = inputs.mean(), inputs.std()\n","        inputs = (inputs - inputs_m) / inputs_s\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Keep stats for Loss and Accuracy\n","        running_loss += loss.item()\n","\n","        # Get the predicted class with the highest score\n","        _, prediction = torch.max(outputs,1)\n","        # Count of predictions that matched the target label\n","        correct_prediction += (prediction == labels).sum().item()\n","        total_prediction += prediction.shape[0]\n","\n","        #if i % 10 == 0:    # print every 10 mini-batches\n","        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","\n","    # Print stats at the end of the epoch\n","    num_batches = len(train_dl)\n","    avg_loss = running_loss / num_batches\n","    acc = correct_prediction/total_prediction\n","    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n","\n","  print('Finished Training')\n","\n","num_epochs=10    # Just for demo, adjust this higher.\n","training(myModel, train_dl, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aC_CtQg_QUEf","executionInfo":{"status":"ok","timestamp":1699278212180,"user_tz":-60,"elapsed":3522568,"user":{"displayName":"Mattia Varagnolo","userId":"04951767506105516001"}},"outputId":"6efcacf0-c32a-45bf-e36a-3476afc0494a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 1.54, Accuracy: 0.51\n","Epoch: 1, Loss: 1.02, Accuracy: 0.61\n","Epoch: 2, Loss: 0.81, Accuracy: 0.68\n","Epoch: 3, Loss: 0.70, Accuracy: 0.72\n","Epoch: 4, Loss: 0.62, Accuracy: 0.75\n","Epoch: 5, Loss: 0.57, Accuracy: 0.77\n","Epoch: 6, Loss: 0.51, Accuracy: 0.80\n","Epoch: 7, Loss: 0.48, Accuracy: 0.82\n","Epoch: 8, Loss: 0.46, Accuracy: 0.82\n","Epoch: 9, Loss: 0.43, Accuracy: 0.84\n","Finished Training\n"]}]}]}